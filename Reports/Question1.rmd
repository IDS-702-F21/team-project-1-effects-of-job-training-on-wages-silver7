---
title: '**Modeling and Representation of Data - Team Project 1**'
author: "John Owusu Duah, Michelle Van, Peining Yang, Sarwari Das, Satvik Kishore"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup, echo=FALSE, include=FALSE}
options(warn = -1)
options(xtable.comment = FALSE)
options(tinytex.verbose = TRUE)
library(xtable)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")
library(ggplot2)
library(MASS)
library(arm)
library(pROC)
library(e1071)
library(caret)
library(rms) #for VIF
library(MASS)
require(gridExtra)
library(sjlabelled)

#knitr::opts_chunk$set(fig.width=10, fig.height=5)
```
# Introduction 

In this project, we analyse data derived from an experiment conducted by the National Supported Work (NSW) Demonstration, in which researchers wanted to assess whether or not job training for disadvantaged workers had an effect on their wages.
Specifically, we seek to understand if workers who receive job training tend to earn higher wages than workers who do not receive job training. We also investigate if this effect differs across demographic groups. Through the course of this analysis, we perform exploratory data analysis to identify the different socioeconomic characteristics that are associated with real annual earnings, and then model them using a linear regression. (Add Findings)

## Data Pre-processing 

We study a subset of the data originally organized in Lalonde, R. J. (1986), which is our main reference for this analysis.  Here, the treatment group includes male participants for which 1974 earnings can be obtained, and the control group includes unemployed males whose income in 1975 was below the poverty level.

Refer to appendix for the full data dictionary. A summary of the continuous variables can be found in Table 1. The following transformations are applied to the  data:
- Combined 'black' and 'Hispanic' into a 'race' variable, where 0 indicates 'other' races, 1 indicates black men and 2 indicates Hispanic men.       
- Created a new variable 'growth', which is the difference between the real earnings in 1978 and 1974.       
- Drop the variable re75; we see that participants were paid during the experiment, and hence the wages recorded in 1975 act as a confounding variable to our outcome (growth). To capture the unique effect of the treatment, we choose to drop re75.       
- Center our age and education variables, for easier interpretation of the intercept.       
```{r, results = "asis",echo=FALSE,header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}
library(stargazer)
dta <- read.csv("/Users/sarwaridas/Desktop/IDS\ 702/TeamProjects/Project1/lalonde.csv",header = TRUE,sep = ",",stringsAsFactors = FALSE)

f_cols= c('treat','black','hispan','married','nodegree') #changing to factor dtype
dta[f_cols] = lapply(dta[f_cols], as.factor) #changing to factor dtype

dta$race <- with(dta, interaction(black, hispan))
dta$race <- factor(as.numeric(dta$race)-1)
# other race set to 1
# dta$race[(dta$race == 0)] <- 1
# # black race set to 2
# dta$race[(dta$race == 1)] <- 2
# # hispanic race set to 3
# dta$race[(dta$race == 2)] <- 3

dta$growth= dta$re78-dta$re74

stargazer(dta,header=FALSE,title="Summary Statistics", type='latex')
dta$treat_as_names= dta$treat
levels(dta$treat_as_names)<- c("Control","Treatment")
#table(dta$treat)
#lapply(dta, function(x){ length(which(x==0))})

#$y_{i}(re78- re74) = \beta _{0} + \beta _{1}x_{i1}(age)+ \beta _{2}x_{i2}(married)+ \\\beta _{3j}x_{i3}(treat)+\beta _{4j}x_{i1}x_{i3}(treat:age)+\beta _{5j}x_{i1}x_{i2}(married:age)$

```
We have 614 records in our final data, with 429 participants in the control group and 185 in the treatment group. We checked for missing data and found none. Large number of zero values are seen in each of the income columns (details in Appendix), but given that the experiment was run on disadvantaged workers lacking job skills, we conclude that these are meaningful to the inferential questions asked and are not being treated like missing values. Outliers are also seen in the data: in re78 for example, the maximum value is more than 5 times the value of the third quartile. We make note of this, and choose to deal with it during our regression modelling. 

## Exploratory Data Analysis

To quantify the effect of a treatment in a randomized experiment, ideally, the treatment and control groups should be balanced. In our data, we see that this is not the case. On running a t-test on baseline income levels (re74) across treat, we see that a significant difference (p<0.001) exists across means: for the control group, baseline income is 5619.24 dollars on average, and for the treatment group, it is 2095.57 dollars on average. This provides justification for using growth in income (re78-re74) instead of final income as our outcome variable. Fig X shows the underlying distribution of our response variable, growth. It is fairly normal, so we don't feel the need to perform any transformations on it. A significant difference (p<0.001) also exists in mean age across the two groups (Fig X). Average age for the control group is 28.03 years, while it is 25.82 years for the treatment group. As age can impact income growth, we take this as a limitation in our analysis. Large differences also exist in the proportion of married people across treatment groups: in the treatment group, only 18.91% of the participants were married, compared to the 51.28% of people in the control group. For education, 70.91% of the treatment group were degree holders, compared to 59.67% of the control group. 

```{r, results = "asis",fig.width=3.2, fig.height=2.4,header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}
library(ggplot2)
ggplot(data=dta, aes(x=growth)) + geom_histogram(aes(y=..density..), color="black", linetype="dashed", bins=20, fill=rainbow(20)) + geom_density(alpha=.25, fill="lightblue") + scale_fill_brewer(palette="Blues") + labs(title="Distribution of Earnings Growth", y="Frequency", x="Growth in Earnings") + theme_classic() + theme(legend.position="none")


ggplot(dta,aes(x=treat_as_names, y=(age), fill=treat_as_names)) +
  geom_boxplot() + #coord_flip() +
  scale_fill_brewer(palette="Blues") +
  labs(title="Age across Treatment groups",x="Experimental group",y="Age") +
  theme_classic() + theme(legend.position="Top")

# ggplot(dta,aes(x=age, y=treat_as_names, fill=treat_as_names)) +
#   geom_boxplot() + #coord_flip() +
#   labs(title="Treatment Versus Age", x="Age", y="Treat") + 
#   theme_classic() + scale_fill_discrete(labels=c("Not Trained","Trained"))


#t.test(growth~treat,data=dta,alternative="greater")

#prop.table(table(dta$treat_as_names,dta$married),margin=1)
#prop.table(table(dta$treat_as_names,dta$nodegree),margin=1)

```
Before the modelling process, we perform an exploratory analysis to understand the plausible relationships between the  predictors (All plots in Appendix).Comparing income growth across predictors, boxplots show us that median growth was higher for participants in the treatment group, compared to the control group. (Fig X) However, on performing a t-test, we see that difference in means were not significant. Median growth was also higher for unmarried people, compared to married people. No significant relationships were seen for degree holders and years of education. Across races, it was seen that median growth was highest for hispanic people, followed by black people and other races. Across age, a decline was noticed in earnings growth as a participant ages.  

We are further interested in knowing whether the treatment effect varies across any demographic groups. On exploring interaction effects across our variables, we notice that trend of earnings growth against age changes  across treatment groups: in the control group, earnings growth reduces as people age, while in the treatment group older people see more income growth. Trend changes are also noticed in the interaction between 
age and married: married people in the treatment group show lower median income growth than unmarried people, but the trend reverses in the control group. We also see some potential interactions for treat with race, and treat with degreeholders.

*Need to fit 2 EDA plots here: plot from our ppt for age vs treat + another*
\newpage

```{r, results = "asis",fig.width=3.5, fig.height=3,header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}
# ggplot(data=dta, aes(x=growth)) + geom_histogram(aes(y=..density..), color="black", linetype="dashed", bins=20, fill=rainbow(20)) + geom_density(alpha=.25, fill="lightblue") + scale_fill_brewer(palette="Blues") + labs(title="Distribution of Earnings Growth", y="Frequency", x="Growth in Earnings") + theme_classic() + theme(legend.position="none")
# 
# 
# ggplot(dta,aes(x=treat_as_names, y=(age), fill=treat_as_names)) +
#   geom_boxplot() + #coord_flip() +
#   scale_fill_brewer(palette="Blues") +
#   labs(title="Age across Treatment groups",x="Experimental group",y="Age") + 
#   theme_classic() + theme(legend.position="Top")

#t.test(growth~treat,data=dta,alternative="greater")

#prop.table(table(dta$treat_as_names,dta$married),margin=1)
#prop.table(table(dta$treat_as_names,dta$nodegree),margin=1)

```

## Model Building: Baseline (Main effects)

To begin, we construct a simple regression model with income growth as the response variable, and the main effects of all variables as our response variables. Age and education are centered.

$$
Growth \sim \beta_0 +  \beta_1 \ treat+  \beta_2 \ age+ \beta_3 \ married+ \beta_4 \ educ+ \beta_5 \ race+ \beta_6 \ nodegree+\epsilon 
$$
A summary for the baseline model can be found in the Appendix. but our main takeaways were: (1) whether or not the participant was in the treatment group was a highly significant predictor (p<0.001) of their income growth (2) age and married are also significant predictors of income growth (3) the model explains about 5% of the variability in income growth.

```{r, echo=FALSE}
dta$age_c <- c(scale(dta$age, scale=F))
dta$educ_c <- c(scale(dta$educ, scale=F))

baselinemodel <- lm(formula=growth ~ treat + age_c + married + educ_c + race + nodegree, data=dta)
#summary(baselinemodel)
```

On plotting the continuous variables ‘age’ and ‘educ’ against growth, we see somewhat linear trends. Transformations like log- transformations and polynomial forms do not improve the residual plots, so we decide to go with the variables as they are. On checking the residual plots, we see that points in the Residuals vs Fitted plot are random, but look clustered in the center. They form a somewhat equal band about the axis, so we conclude that both independence and constant variance are not violated, but can be improved. In the Q-Q plot, points are on the 45 degree line, but we see deviations on both ends. We carried out several iterations of transformations of the variables to improve the condition of normality in the error term but no noticeable improvement was found. Since the points do not sharply deviate from the 45 degree line, we conclude that our model does not violate the normality assumption. On checking the Scale-Location and standardized residuals vs leverage plot, we see that we have outliers, but they are not influential. Hence we do not decide to remove them. To improve our model, we move on to the model selection process.
```{r, results = "asis",fig.width=4, fig.height=3.8, fig.show="hold", out.width="32%", header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}

plot(baselinemodel,which=1, col=c("darkblue"),sub.caption = "")
plot(baselinemodel,which=2, col=c("darkblue"),sub.caption = "")
#plot(m,which=3, col=c("darkblue"),sub.caption = "")
plot(baselinemodel,which=5, col=c("darkblue"),sub.caption = "")
```

## Model Selection: Final (Forward Model Selection with AIC)

For our model selection we construct a null model having just treat as the response variable, and a full model having all main effects as well as all possible interaction terms for treat, age and race and education. Using AIC (**GIVE REASON WHY**), forward selection gives us the final model given below.
```{r, echo=FALSE}
nullmodel <- lm(formula = growth ~ treat, data=dta)
fullmodel <- lm(formula = growth ~ treat + educ_c + race + nodegree + married  + educ_c*(treat + age_c + race + nodegree + married) + treat*(race + nodegree + married + age_c) + race*(nodegree + married) + age_c:married + nodegree:married, data=dta)
stepwise_aic_model <- step(nullmodel, scope=formula(fullmodel), direction="both", trace=0)
selectedmodel= stepwise_aic_model
#summary(selectedmodel)
```

$$
Growth \sim \beta_0 +  \beta_1 \ treat+  \beta_2 \ age+ \beta_3 \ married+ \beta_4 \ married:age+ \beta_5 \ treat:age+\epsilon ;     \epsilon \sim N(0,\sigma^2)
$$

Table X gives a summary for the model. We assess linearity by plotting ‘age’ and ‘educ’ against growth and see a linear trend. Thus the linearity assumption is met. On checking the residual plots, we see that points in the Residuals vs Fitted plot are random, but look clustered in two groups. Since the independence assumption requires independence in the Y-axis and does not regard patterns on the X axis, we conclude that the independence assumption is not violated. However, the presence of the clusters indicate the presence of an omitted variable, which we count as a limitation in our analysis. The points seem to form an equal band around the axis, so we conclude that the constant variance assumption is met. Checking the Q-Q plot, we see that more points are on the 45 degree line than the baseline model, although deviations still exist. We conclude that normality of residuals have improved, and the assumption has been met. On checking the Scale-Location and standardized residuals vs leverage plots, we see outliers (especially point 132). However, these are not influential so we choose to not remove them. Finally, on checking for multicollinearity, we see that the highest Variance Inflation Factor (VIF) was 1.23, which is substantially less than the threshold for concern. 

```{r, results = "asis",fig.width=4, fig.height=3.8, fig.show="hold", out.width="32%", header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}

plot(selectedmodel,which=1, col=c("darkblue"),sub.caption = "")
plot(selectedmodel,which=2, col=c("darkblue"),sub.caption = "")
#plot(m,which=3, col=c("darkblue"),sub.caption = "")
plot(selectedmodel,which=5, col=c("darkblue"),sub.caption = "")
```

## Model Interpretation


```{r, results = "asis",header= FALSE, message = FALSE, warning = FALSE, echo=FALSE}
library(stargazer)
stargazer(selectedmodel,title= "Results", header=FALSE, type='latex',digits = 2,no.space = TRUE,column.sep.width = "3pt",single.row=TRUE)

```




